{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# NLP Homework 4 Programming Assignment\n",
    "\n",
    "In this assignment, we will train and evaluate a neural model to tag the parts of speech in a sentence.\n",
    "We will also implement several improvements to the model to test its performance.\n",
    "\n",
    "We will be using English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "## Building a POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X367eCR3_x0"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "### Preparing Data\n",
    "We collect the data in the following cell from the `train.txt` and `test.txt` files.  \n",
    "For `train.txt`, we read the word and tag sequences for each sentence. We then create an 80-20 train-val split on this data for training and evaluation purpose.\n",
    "\n",
    "Finally, we are interested in our accuracy on `test.txt`, so we prepare test data from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "kFpH2P1A3_yG",
    "outputId": "1c889944-290e-4231-9b34-8c07f777aaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  7148\n",
      "Val Data:  1788\n",
      "Test Data:  2012\n",
      "Total tags:  44\n"
     ]
    }
   ],
   "source": [
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "def load_txt_data(txt_file):\n",
    "    all_sentences = []\n",
    "    sent = []\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if(line.strip() == \"\"):\n",
    "                all_sentences.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                word = line.strip()\n",
    "                sent.append(word)\n",
    "    return all_sentences\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "test_sentences = load_txt_data('test.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Test Data: \", len(test_sentences))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uojEDun83_yP",
    "outputId": "fb218599-7c4b-4b67-cf4d-929e2e8ce2d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 21589\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "### Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 25\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. \n",
    "\n",
    "These word embeddings act as input to the LSTM which produces a hidden state. This hidden state is then passed to a Linear layer that produces the probability distribution for the tags of every word. The model will output the tag with the highest probability for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCa30HQb3_ym"
   },
   "outputs": [],
   "source": [
    "class BasicPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = self.hidden_dim)\n",
    "        self.linear = nn.Linear(in_features = self.hidden_dim, out_features = tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        batch_size = sentence.shape[0]\n",
    "        self.hidden_cell = (torch.zeros(1, batch_size, self.hidden_dim),\n",
    "                             torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        \n",
    "        embedding = self.embedding(sentence)\n",
    "        embedding = embedding.unsqueeze(-1).permute(2, 0, 1)\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(embedding, self.hidden_cell)\n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        tag_scores = self.linear(lstm_out)\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "        tags_new = torch.as_tensor(prepare_sequence(tags, tag_to_idx))\n",
    "        \n",
    "        pred_tags = model(sentence_new)\n",
    "        \n",
    "        loss = loss_function(pred_tags, tags_new)\n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(sentence_new)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "            tags_new = torch.as_tensor(prepare_sequence(tags, tag_to_idx))\n",
    "            pred_tags = model(sentence_new)\n",
    "            \n",
    "            loss = loss_function(pred_tags, tags_new)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred_tags = torch.argmax(pred_tags, dim = 1)\n",
    "            correct += sum(pred_tags == tags_new).item()\n",
    "            val_examples += len(sentence_new)\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsuHjjH1rQeS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.0798\tAvg Val Loss: 0.0558\t Val Accuracy: 65\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0469\tAvg Val Loss: 0.0420\t Val Accuracy: 74\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0368\tAvg Val Loss: 0.0350\t Val Accuracy: 78\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0306\tAvg Val Loss: 0.0301\t Val Accuracy: 82\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0259\tAvg Val Loss: 0.0266\t Val Accuracy: 84\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0223\tAvg Val Loss: 0.0239\t Val Accuracy: 86\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0195\tAvg Val Loss: 0.0219\t Val Accuracy: 87\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0172\tAvg Val Loss: 0.0204\t Val Accuracy: 88\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0154\tAvg Val Loss: 0.0192\t Val Accuracy: 88\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0139\tAvg Val Loss: 0.0182\t Val Accuracy: 89\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0127\tAvg Val Loss: 0.0175\t Val Accuracy: 89\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0116\tAvg Val Loss: 0.0168\t Val Accuracy: 90\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0108\tAvg Val Loss: 0.0163\t Val Accuracy: 90\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0100\tAvg Val Loss: 0.0159\t Val Accuracy: 90\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0094\tAvg Val Loss: 0.0155\t Val Accuracy: 90\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0088\tAvg Val Loss: 0.0152\t Val Accuracy: 91\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0083\tAvg Val Loss: 0.0150\t Val Accuracy: 91\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0079\tAvg Val Loss: 0.0147\t Val Accuracy: 91\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0075\tAvg Val Loss: 0.0146\t Val Accuracy: 91\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0072\tAvg Val Loss: 0.0144\t Val Accuracy: 91\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0069\tAvg Val Loss: 0.0143\t Val Accuracy: 91\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0066\tAvg Val Loss: 0.0142\t Val Accuracy: 91\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0064\tAvg Val Loss: 0.0141\t Val Accuracy: 91\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0062\tAvg Val Loss: 0.0140\t Val Accuracy: 91\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0060\tAvg Val Loss: 0.0140\t Val Accuracy: 91\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0059\tAvg Val Loss: 0.0139\t Val Accuracy: 91\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0057\tAvg Val Loss: 0.0139\t Val Accuracy: 92\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0056\tAvg Val Loss: 0.0138\t Val Accuracy: 92\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0055\tAvg Val Loss: 0.0138\t Val Accuracy: 92\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0054\tAvg Val Loss: 0.0138\t Val Accuracy: 92\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "basicpos_model = BasicPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx))\n",
    "basicpos_optimizer = optim.SGD(basicpos_model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, basicpos_model, loss_function, basicpos_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "You should get a performance of **at least 80%** on the validation set for the BasicPOSTagger.\n",
    "\n",
    "Let us now write a method to save our predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e9aoWNcNomd"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "            pred_tags = basicpos_model(sentence_new)            \n",
    "            pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "            for tag_id in pred_tags:\n",
    "                predicted_tags.append(idx_to_tag[tag_id])\n",
    "            \n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azW08GfZSHcQ"
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9T6s3XTFG46"
   },
   "source": [
    "\n",
    "### Test accuracy\n",
    "Evaluate your performance on the test data by submitting test_labels.txt generated by the method above and **report your test accuracy here**.\n",
    "\n",
    "The accuracy I achieved on gradescope was 86.7172% (team name is \"browns 4 superbowl'')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iP64WDReBuDr"
   },
   "source": [
    "Imitate the above method to generate prediction for validation data.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Use these lists to carry out error analysis to find the top-10 types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QgMHr7HCn1x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 181\t| ['licensed', 'decided', 'tested', 'snapped', 'restructured']\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| NN\t\t| 167\t| ['appeal', 'buy-back', 'factor', 'view', 'survey']\n",
      "-------------------------------------------------------------------------\n",
      "VBD\t| VBN\t\t| 156\t| ['hurt', 'deprived', 'directed', 'acquired', 'alleged']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| NN\t\t| 142\t| ['Trinen', 'Anglo', 'Cologne', 'Protestantism', 'Monorail']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| VB\t\t| 135\t| ['lead', 'report', 'fare', 'hold', 'shield']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 129\t| ['past', 'chief', 'weekly', 'hazardous-waste', 'colleague']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 126\t| ['third-ranking', 'fetal-tissue', 'decade-long', 'unoccupied', 'Palestinian']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| NNP\t\t| 121\t| ['dish', 'Business', 'festival', 'blip', 'theater']\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| VBP\t\t| 109\t| ['exclude', 'seem', 'fly', 'believe', 'want']\n",
      "-------------------------------------------------------------------------\n",
      "VBP\t| VB\t\t| 97\t| ['follow', 'lag', 'occur', 'halt', 'sell']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, val_data):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    \n",
    "    for sentence, tags in val_data:\n",
    "        sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "        pred_tags = model(sentence_new)\n",
    "        pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "        pred_tags = [idx_to_tag[tag_id] for tag_id in pred_tags]\n",
    "        \n",
    "        word_list += sentence\n",
    "        model_tags += pred_tags\n",
    "        gt_tags += tags\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    errors = {}\n",
    "    for index, model_tag in enumerate(model_tags):\n",
    "        gt_tag = gt_tags[index]\n",
    "        word = word_list[index]\n",
    "        if model_tag == gt_tag:\n",
    "            continue\n",
    "        if (model_tag, gt_tag) not in errors:\n",
    "            errors[(model_tag, gt_tag)] = (0, set())\n",
    "        curr_count, curr_word_list = errors[(model_tag, gt_tag)]\n",
    "        curr_word_list.add(word)\n",
    "        errors[(model_tag, gt_tag)] = (curr_count + 1, curr_word_list)\n",
    "    errors = sorted(errors.items(), key = lambda x : x[1][0], reverse = True)[0:10]\n",
    "    return errors\n",
    "\n",
    "basicpos_word_list, basicpos_model_tags, basicpos_gt_tags = generate_predictions(basicpos_model, val_data)\n",
    "basicpos_errors = error_analysis(basicpos_word_list, basicpos_model_tags, basicpos_gt_tags)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "print(horizontal_line)\n",
    "for error in basicpos_errors:\n",
    "    tags, info = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = info\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, 5), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "What kinds of errors did the model make and why do you think it made them?\n",
    "\n",
    "The top 3 and bottom 2 most frequent errors this model made had to deal with verb tags (VBN, VB, VBD, VB, VBP). In fact, VB shows up twice in tha tlist. Similarly, NN appears 3 times are the ground truth tag in our list. Meanwhile, there are no confusions in teams of pronouns, symbols, and prepositions. This is interesting, but sort of makes sense. For example, the difference between VBN (verb, past participle) and VBD (verb, past tense) is marginal. That's such a small subtlety. Similarly, the difference between NN (noun, singular or mass) and NNP (proper noun, singular) is marginal as well. Technically, NNP is a subet of NN, as all proper nouns are also nouns. I think the only true, poor errors made by the model are classifying NNs (nouns) as JJs (adjectives) and JJs as NNPs (proper noun, singular), as those are vastly different part of speech categories (one describes an object, while the other is the object). However, when investigating the example words closely, they kind of make sense. For example, \"chief\" can be a tribal chief, which is a singular noun, or it can mean imply something is the \"chief cause\" of something, or primary cause in other words. As such, similar reasoning can be used for \"hazardous-waste\", \"past\", and others. \n",
    "\n",
    "In general, I think the model did a pretty decent job with the POS tagging. It achieved high train, validation, and test accuracy, and a significant majority of its errors came from two tag types that are similar (e.g. VBN and VBD, or NNP and NN). Most of these errors came from abiguity in the tags themselves, where I think humans would make very similar errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## Define a Character Level POS Tagger\n",
    "\n",
    "We can use the character-level information present to augment our word embeddings. Words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, we can run a character level LSTM on every word (treated as a tensor of characters, each mapped to character-index space) to create a character-level representation of the word. This representation can be concatenated with the word embedding (as in the BasicPOSTagger) to create a new word embedding that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n",
    "\n",
    "# New Hyperparameters\n",
    "EMBEDDING_DIM = 20\n",
    "HIDDEN_DIM = 25\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30\n",
    "CHAR_EMBEDDING_DIM = 10\n",
    "CHAR_HIDDEN_DIM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "class CharPOSTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an char level LSTM: that finds the character level embedding for a word\n",
    "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.char_hidden_dim = char_hidden_dim\n",
    "        self.char_size = char_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(num_embeddings = char_size, embedding_dim = self.char_embedding_dim)\n",
    "        self.word_embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = self.embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(input_size = self.char_embedding_dim, hidden_size = self.char_hidden_dim)\n",
    "        self.joint_lstm = nn.LSTM(input_size = self.embedding_dim + self.char_hidden_dim, hidden_size = self.hidden_dim)\n",
    "        self.linear = nn.Linear(in_features = self.hidden_dim, out_features = tagset_size)\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        word_batch_size = sentence.shape[0]\n",
    "        self.joint_hidden_cell = (torch.zeros(1, word_batch_size, self.hidden_dim),\n",
    "                             torch.zeros(1, word_batch_size, self.hidden_dim))\n",
    "        \n",
    "        word_char_embeddings = torch.Tensor([])\n",
    "        for word_chars in chars:\n",
    "            char_batch_size = len(word_chars)\n",
    "            self.char_hidden_cell = (torch.zeros(1, 1, self.char_hidden_dim),\n",
    "                             torch.zeros(1, 1, self.char_hidden_dim))\n",
    "            \n",
    "            char_embedding = self.char_embedding(word_chars)\n",
    "            char_embedding = char_embedding.unsqueeze(1)\n",
    "            char_lstm_out, _ = self.char_lstm(char_embedding, self.char_hidden_cell)\n",
    "            char_lstm_out = char_lstm_out[-1]\n",
    "            word_char_embeddings = torch.cat((word_char_embeddings, char_lstm_out), dim = 0)\n",
    "        word_char_embeddings = word_char_embeddings.unsqueeze(0)\n",
    "        \n",
    "        sentence_embedding = self.word_embedding(sentence)\n",
    "        sentence_embedding = sentence_embedding.unsqueeze(0)\n",
    "        \n",
    "        joint_embedding = torch.cat((sentence_embedding, word_char_embeddings), dim = 2)\n",
    "        \n",
    "        joint_lstm_out, _ = self.joint_lstm(joint_embedding, self.joint_hidden_cell)\n",
    "        joint_lstm_out = joint_lstm_out.squeeze(0)\n",
    "        tag_scores = self.linear(joint_lstm_out)\n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences as well as character sequences. Find the gradient with \n",
    "        # respect to the loss and update the model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        #print(sentence)\n",
    "        chars_new = [prepare_sequence([char for char in word], char_to_idx) for word in sentence]\n",
    "        #print(chars_new)\n",
    "        sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "        tags_new = prepare_sequence(tags, tag_to_idx)\n",
    "        #print(\"tags new: \", tags_new)\n",
    "        \n",
    "        pred_tags = model(sentence_new, chars_new)\n",
    "        #print(\"pred tags: \", pred_tags)\n",
    "        #print(\"tags pred new: \", tags_new)\n",
    "        \n",
    "        loss = loss_function(pred_tags, tags_new)\n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(sentence)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function, optimizer)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate_char(model, loss_function, optimizer):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            chars_new = [prepare_sequence([char for char in word], char_to_idx) for word in sentence]\n",
    "            sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "            tags_new = prepare_sequence(tags, tag_to_idx)\n",
    "\n",
    "            pred_tags = model(sentence_new, chars_new)\n",
    "\n",
    "            loss = loss_function(pred_tags, tags_new)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred_tags = torch.argmax(pred_tags, dim = 1)\n",
    "            correct += sum(pred_tags == tags_new).item()\n",
    "            val_examples += len(sentence)\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-QttCw6Otf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.0632\tAvg Val Loss: 0.0387\t Val Accuracy: 73\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0320\tAvg Val Loss: 0.0285\t Val Accuracy: 80\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0228\tAvg Val Loss: 0.0292\t Val Accuracy: 79\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0215\tAvg Val Loss: 0.0192\t Val Accuracy: 87\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0169\tAvg Val Loss: 0.0172\t Val Accuracy: 89\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0152\tAvg Val Loss: 0.0160\t Val Accuracy: 90\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0141\tAvg Val Loss: 0.0152\t Val Accuracy: 90\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0132\tAvg Val Loss: 0.0146\t Val Accuracy: 91\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0125\tAvg Val Loss: 0.0142\t Val Accuracy: 91\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0118\tAvg Val Loss: 0.0140\t Val Accuracy: 91\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0113\tAvg Val Loss: 0.0135\t Val Accuracy: 91\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0108\tAvg Val Loss: 0.0132\t Val Accuracy: 91\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0104\tAvg Val Loss: 0.0130\t Val Accuracy: 92\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0100\tAvg Val Loss: 0.0127\t Val Accuracy: 92\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0097\tAvg Val Loss: 0.0125\t Val Accuracy: 92\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0096\tAvg Val Loss: 0.0125\t Val Accuracy: 92\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0093\tAvg Val Loss: 0.0122\t Val Accuracy: 92\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0089\tAvg Val Loss: 0.0121\t Val Accuracy: 92\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0087\tAvg Val Loss: 0.0120\t Val Accuracy: 92\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0084\tAvg Val Loss: 0.0119\t Val Accuracy: 92\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0082\tAvg Val Loss: 0.0118\t Val Accuracy: 93\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0087\tAvg Val Loss: 0.0129\t Val Accuracy: 92\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0085\tAvg Val Loss: 0.0118\t Val Accuracy: 92\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0079\tAvg Val Loss: 0.0117\t Val Accuracy: 93\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0077\tAvg Val Loss: 0.0116\t Val Accuracy: 93\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0075\tAvg Val Loss: 0.0115\t Val Accuracy: 93\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0074\tAvg Val Loss: 0.0115\t Val Accuracy: 93\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0072\tAvg Val Loss: 0.0114\t Val Accuracy: 93\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0071\tAvg Val Loss: 0.0114\t Val Accuracy: 93\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0070\tAvg Val Loss: 0.0115\t Val Accuracy: 93\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "charpos_model = CharPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(unique_chars), len(word_to_idx), len(tag_to_idx))\n",
    "charpos_optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train_char(epoch, charpos_model, loss_function, charpos_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            chars_new = [prepare_sequence([char for char in word], char_to_idx) for word in sentence]\n",
    "            sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "            pred_tags = model(sentence_new, chars_new)            \n",
    "            pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "            for tag_id in pred_tags:\n",
    "                predicted_tags.append(idx_to_tag[tag_id])\n",
    "            \n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "Tune your hyperparameters, to get a performance of **at least 85%** on the validation set for the CharPOSTagger.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n",
    "The test accuracy I achieved on Gradescope was 92.4626%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du0raTJreqT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| NN\t\t| 235\t| ['mention', 'stress', 'bite', 'check', 'answer']\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 229\t| ['greeted', 'thought', 'studied', 'soared', 'confirmed']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 220\t| ['History', 'host', 'peso', 'relative', 'mettle']\n",
      "-------------------------------------------------------------------------\n",
      "VBD\t| VBN\t\t| 189\t| ['replaced', 'exonerated', 'projected', 'paid', 'survived']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| VB\t\t| 163\t| ['probe', 'halt', 'austerity', 'face', 'journey']\n",
      "-------------------------------------------------------------------------\n",
      "VBP\t| VB\t\t| 130\t| ['take', 'feel', 'participate', 'run', 'find']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 111\t| ['Structural', 'Initial', 'Medical', 'Durable', 'Soviet-style']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NN\t\t| 105\t| ['hopeful', 'charming', 'stepped-up', 'standby', 'data-storage']\n",
      "-------------------------------------------------------------------------\n",
      "WDT\t| IN\t\t| 93\t| ['that']\n",
      "-------------------------------------------------------------------------\n",
      "VBZ\t| NNS\t\t| 90\t| ['rubs', 'notes', 'accompanies', 'declines', 'portrays']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, val_data):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    \n",
    "    for sentence, tags in val_data:\n",
    "        chars_new = [prepare_sequence([char for char in word], char_to_idx) for word in sentence]\n",
    "        sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "        pred_tags = model(sentence_new, chars_new)\n",
    "        pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "        pred_tags = [idx_to_tag[tag_id] for tag_id in pred_tags]\n",
    "        \n",
    "        word_list += sentence\n",
    "        model_tags += pred_tags\n",
    "        gt_tags += tags\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    errors = {}\n",
    "    for index, model_tag in enumerate(model_tags):\n",
    "        gt_tag = gt_tags[index]\n",
    "        word = word_list[index]\n",
    "        if model_tag == gt_tag:\n",
    "            continue\n",
    "        if (model_tag, gt_tag) not in errors:\n",
    "            errors[(model_tag, gt_tag)] = (0, set())\n",
    "        curr_count, curr_word_list = errors[(model_tag, gt_tag)]\n",
    "        curr_word_list.add(word)\n",
    "        errors[(model_tag, gt_tag)] = (curr_count + 1, curr_word_list)\n",
    "    errors = sorted(errors.items(), key = lambda x : x[1][0], reverse = True)[0:10]\n",
    "    return errors\n",
    "\n",
    "charpos_model = model\n",
    "charpos_word_list, charpos_model_tags, charpos_gt_tags = generate_predictions(charpos_model, val_data)\n",
    "charpos_errors = error_analysis(charpos_word_list, charpos_model_tags, charpos_gt_tags)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "print(horizontal_line)\n",
    "for error in charpos_errors:\n",
    "    tags, info = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = info\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, min(len(example_words), 5)), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GEP-IgiESzN"
   },
   "source": [
    "\n",
    "**Report your findings here.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? \n",
    "\n",
    "The character model's second most frequent error, classified VBN tags as VBD, is the original model's most frequent error, so that's an interesting similarity. I suspect this will be a trend, even for the third model we train below.\n",
    "\n",
    "However, one huge difference is the diversity in tag errors. In the original model, most of the errors came from verb errors (VBN, VB, VBD, VB, VBP), and then noun errors. While many of those errors still persist in this model, this model now frequently makes errors on a new tag, VBZ (verb, 3rd person singular present), more adjective tag errors, and another new tag, WDT (Wh-determiner). This last tag error, WDT, makes sense, because the model predicted it as IN (preposition or subordinating conjunction), which is definitely a possibility for the word \"that\". I think the real reason \"that\" is classified as \"IN\" instead of \"WDT\" is because 1) the word itself is pretty useless in the context of the entire sentence, so if the model had to make an error somewhere, it learned to make the error on the least useful words, such as \"that\", or 2) adding the character level embeddings might have placed too much emphasis on the \"hat\" substring of \"that\", and if the model has seen the word \"what\" a lot, which is different by 1 letter, then it could have been tricked into thinking \"that\" is similar to \"what\".\n",
    "\n",
    "In general, I think the general trend is the same as the original model. This model does a poor job on ambiguous tags, for example within-group tags VBD and VBN. In fact, many of the same errors from the original model persisted in this model, but some were replaced for new errors on new tags, which is due to the character embeddings possibly placing too much emphasis/bias on the wrong portion of the word, as discussed in the above paragraph with \"what\" and \"that\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQdc3gH8d_a4"
   },
   "source": [
    "## Define a BiLSTM POS Tagger\n",
    "\n",
    "A bidirectional LSTM that runs both left-to-right and right-to-left to represent dependencies between adjacent words in both directions and thus captures dependencies in both directions. \n",
    "\n",
    "In this part, you make your model bidirectional. \n",
    "\n",
    "In addition, you should implement one of these modifications to improve the model's performance:\n",
    "- Tune the model hyperparameters. Try at least 5 different combinations of parameters. For example:\n",
    "    - number of LSTM layers\n",
    "    - number of hidden dimensions\n",
    "    - number of word embedding dimensions\n",
    "    - dropout rate\n",
    "    - learning rate\n",
    "- Switch to pre-trained Word Embeddings instead of training them from scratch. Try at least one different embedding method. For example:\n",
    "    - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "    - [Fast Text](https://fasttext.cc/docs/en/english-vectors.html)\n",
    "- Implement a different model architecture. Try at least one different architecture. For example:\n",
    "    - adding a conditional random field on top of the LSTM\n",
    "    - adding Viterbi decoding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo-TWvcYeHhT"
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    # NOTE: you may have to modify these function headers to include your \n",
    "    # modification, e.g. adding a parameter for embeddings data\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, lstm_layers, dropout_rate):\n",
    "        super(BiLSTMPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # a BiLSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size = self.embedding_dim, hidden_size = self.hidden_dim, num_layers = lstm_layers, dropout = dropout_rate, bidirectional = True)\n",
    "        self.linear = nn.Linear(in_features = self.hidden_dim*2, out_features = tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        batch_size = sentence.shape[0]\n",
    "        self.hidden_cell = (torch.zeros(self.lstm_layers*2, batch_size, self.hidden_dim),\n",
    "                             torch.zeros(self.lstm_layers*2, batch_size, self.hidden_dim))\n",
    "        \n",
    "        embedding = self.embedding(sentence)\n",
    "        embedding = embedding.unsqueeze(-1).permute(2, 0, 1)\n",
    "        \n",
    "        lstm_out, self.hidden_cell = self.lstm(embedding, self.hidden_cell)\n",
    "        lstm_out = lstm_out.squeeze(0)\n",
    "        tag_scores = self.linear(lstm_out)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mc2d_0k6mktK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\tAvg Train Loss: 0.0761\tAvg Val Loss: 0.0530\t Val Accuracy: 68\n",
      "Epoch: 2/30\tAvg Train Loss: 0.0448\tAvg Val Loss: 0.0405\t Val Accuracy: 75\n",
      "Epoch: 3/30\tAvg Train Loss: 0.0357\tAvg Val Loss: 0.0342\t Val Accuracy: 79\n",
      "Epoch: 4/30\tAvg Train Loss: 0.0301\tAvg Val Loss: 0.0298\t Val Accuracy: 82\n",
      "Epoch: 5/30\tAvg Train Loss: 0.0258\tAvg Val Loss: 0.0265\t Val Accuracy: 84\n",
      "Epoch: 6/30\tAvg Train Loss: 0.0224\tAvg Val Loss: 0.0239\t Val Accuracy: 86\n",
      "Epoch: 7/30\tAvg Train Loss: 0.0197\tAvg Val Loss: 0.0219\t Val Accuracy: 87\n",
      "Epoch: 8/30\tAvg Train Loss: 0.0175\tAvg Val Loss: 0.0204\t Val Accuracy: 88\n",
      "Epoch: 9/30\tAvg Train Loss: 0.0157\tAvg Val Loss: 0.0191\t Val Accuracy: 88\n",
      "Epoch: 10/30\tAvg Train Loss: 0.0142\tAvg Val Loss: 0.0181\t Val Accuracy: 89\n",
      "Epoch: 11/30\tAvg Train Loss: 0.0130\tAvg Val Loss: 0.0173\t Val Accuracy: 89\n",
      "Epoch: 12/30\tAvg Train Loss: 0.0119\tAvg Val Loss: 0.0166\t Val Accuracy: 90\n",
      "Epoch: 13/30\tAvg Train Loss: 0.0110\tAvg Val Loss: 0.0161\t Val Accuracy: 90\n",
      "Epoch: 14/30\tAvg Train Loss: 0.0102\tAvg Val Loss: 0.0156\t Val Accuracy: 90\n",
      "Epoch: 15/30\tAvg Train Loss: 0.0096\tAvg Val Loss: 0.0152\t Val Accuracy: 91\n",
      "Epoch: 16/30\tAvg Train Loss: 0.0090\tAvg Val Loss: 0.0149\t Val Accuracy: 91\n",
      "Epoch: 17/30\tAvg Train Loss: 0.0085\tAvg Val Loss: 0.0146\t Val Accuracy: 91\n",
      "Epoch: 18/30\tAvg Train Loss: 0.0080\tAvg Val Loss: 0.0143\t Val Accuracy: 91\n",
      "Epoch: 19/30\tAvg Train Loss: 0.0077\tAvg Val Loss: 0.0141\t Val Accuracy: 91\n",
      "Epoch: 20/30\tAvg Train Loss: 0.0073\tAvg Val Loss: 0.0140\t Val Accuracy: 91\n",
      "Epoch: 21/30\tAvg Train Loss: 0.0070\tAvg Val Loss: 0.0138\t Val Accuracy: 91\n",
      "Epoch: 22/30\tAvg Train Loss: 0.0068\tAvg Val Loss: 0.0137\t Val Accuracy: 91\n",
      "Epoch: 23/30\tAvg Train Loss: 0.0065\tAvg Val Loss: 0.0136\t Val Accuracy: 91\n",
      "Epoch: 24/30\tAvg Train Loss: 0.0063\tAvg Val Loss: 0.0135\t Val Accuracy: 91\n",
      "Epoch: 25/30\tAvg Train Loss: 0.0061\tAvg Val Loss: 0.0134\t Val Accuracy: 92\n",
      "Epoch: 26/30\tAvg Train Loss: 0.0060\tAvg Val Loss: 0.0133\t Val Accuracy: 92\n",
      "Epoch: 27/30\tAvg Train Loss: 0.0058\tAvg Val Loss: 0.0133\t Val Accuracy: 92\n",
      "Epoch: 28/30\tAvg Train Loss: 0.0057\tAvg Val Loss: 0.0132\t Val Accuracy: 92\n",
      "Epoch: 29/30\tAvg Train Loss: 0.0055\tAvg Val Loss: 0.0132\t Val Accuracy: 92\n",
      "Epoch: 30/30\tAvg Train Loss: 0.0054\tAvg Val Loss: 0.0132\t Val Accuracy: 92\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "EMBEDDING_DIM = 500\n",
    "HIDDEN_DIM = 450\n",
    "LSTM_LAYERS = 1\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.0\n",
    "\n",
    "bilstm_model = BiLSTMPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_idx), len(tag_to_idx), LSTM_LAYERS, DROPOUT)\n",
    "bilstm_optimizer = optim.SGD(bilstm_model.parameters(), lr = LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, bilstm_model, loss_function, bilstm_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    predicted_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence in test_sentences:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the test loop\n",
    "            # This method saves the predicted tags for the sentences in the test set.\n",
    "            # The tags are first added to a list which is then written to a file for\n",
    "            # submission. An empty string is added after every sequence of tags\n",
    "            # corresponding to a sentence to add a newline following file formatting\n",
    "            # convention, as has been done already.\n",
    "            #############################################################################\n",
    "            sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "            pred_tags = bilstm_model(sentence_new)            \n",
    "            pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "            for tag_id in pred_tags:\n",
    "                predicted_tags.append(idx_to_tag[tag_id])\n",
    "            \n",
    "            predicted_tags.append(\"\")\n",
    "\n",
    "    with open('test_labels.txt', 'w+') as f:\n",
    "        for item in predicted_tags:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S6NwhSXaBCNl"
   },
   "source": [
    "Your modified model should get a performance of **at least 90%** on the validation set.\n",
    "\n",
    "### Test accuracy\n",
    "Also evaluate your performance on the test data by submitting test_labels.txt and **report your test accuracy here**.\n",
    "\n",
    "I went ahead and chose the path to tune the hyperparameters. I went through 11 different combinations of hyperparameters with the following validation accuracies:\n",
    "\n",
    "| Embedding dim | Hidden dim | Num LSTM layers | Learning rate | Dropout | Val. Acc. |\n",
    "|---------------|------------|-----------------|---------------|---------|-----------|\n",
    "| 20            | 25         | 2               | 0.1           | 0.5     | 79%       |\n",
    "| 35            | 40         | 4               | 0.1           | 0.1     | 87%       |\n",
    "| 35            | 40         | 4               | 0.1           | 0.0     | 91%       |\n",
    "| 35            | 40         | 4               | 0.2           | 0.0     | 91%       |\n",
    "| 35            | 40         | 1               | 0.1           | 0.0     | 91%       |\n",
    "| 60            | 70         | 1               | 0.1           | 0.0     | 91%       |\n",
    "| 100           | 150        | 1               | 0.1           | 0.0     | 92%       |\n",
    "| 100           | 150        | 1               | 0.075         | 0.0     | 91%       |\n",
    "| 100           | 150        | 2               | 0.075         | 0.05    | 91%       |\n",
    "| 300           | 150        | 1               | 0.1           | 0.0     | 92%       |\n",
    "| 500           | 450        | 1               | 0.01          | 0.0     | 92%       |\n",
    "\n",
    "Now, I've hit the 90% mark on the validation set, however, every time I submit my predictions on the Gradescope test set, the test accuracy is largely hanging around 89.75%. From the above simulations, I found a larger number of LSTM layers actually has no effect on performance or it has a negative effect. I suspect this is due to overfitting. A lower learning rate only really helped me get a lower average validation loss, while still maintaining that 92% validation accuracy. Additionally, I noticed huge improvements in the model accuracy as the embedding and hidden dimensions went up. This makes sense because the model is able to embed more information per word, thus increasing inter-word separability. Finally, dropout was almost useless. I tried several values and its effect was almost completely useless. In fact, as I dropped the dropout rate, validation accuracy went up. I suspect this is because the model doesn't have an insane number of parameters, so performing dropout on large chunks might ruin the model, especially when comparing to larger deep learning models, such as ResNet.\n",
    "\n",
    "In general, the best model is either the one with embedding dim 100, hidden dim 150, and val accuracy 92%, or the one with embedding dim 500 and hidden dim 450 with val accuracy 92%. When I plug the former's predictions into Gradescope, I get a test accuracy of 89.75%, and the latter is 89.57%. However, the former has a slightly higher average validation loss (0.0156 compared to 0.0132). So, I'd say the one with the larger embedding dim will probably be better in the long-term, especially if we decide to add more words in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kh0S5yXIA_0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt_tag\t| model_tag\t| freq.\t| examples\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| NN\t\t| 173\t| ['cooperate', 'defeat', 'block', 'rank', 'time']\n",
      "-------------------------------------------------------------------------\n",
      "VBN\t| VBD\t\t| 168\t| ['signed', 'helped', 'destroyed', 'executed', 'realized']\n",
      "-------------------------------------------------------------------------\n",
      "VBD\t| VBN\t\t| 156\t| ['singled', 'referred', 'opposed', 'hired', 'upheld']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| VB\t\t| 138\t| ['scrap', 'wonder', 'finance', 'disturbance', 'thunder']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| JJ\t\t| 134\t| ['misconduct', 'current', 'upsurge', 'donnybrook', 'common']\n",
      "-------------------------------------------------------------------------\n",
      "NNP\t| NN\t\t| 129\t| ['Consumer', 'Keihin', 'Comanche', 'FREDERICK', 'Assessment']\n",
      "-------------------------------------------------------------------------\n",
      "NN\t| NNP\t\t| 120\t| ['overflow', 'assortment', 'sweetener', 'grade', 'dustbin']\n",
      "-------------------------------------------------------------------------\n",
      "JJ\t| NNP\t\t| 120\t| ['Medical', 'Milan-based', '30-share', 'rugged', '150-plus']\n",
      "-------------------------------------------------------------------------\n",
      "VB\t| VBP\t\t| 113\t| ['assume', 'say', 'exclude', 'think', 'expire']\n",
      "-------------------------------------------------------------------------\n",
      "VBP\t| VB\t\t| 100\t| ['lose', 'regret', 'rely', 'participate', 'try']\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions from val data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "#############################################################################\n",
    "def generate_predictions(model, val_data):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    word_list = []\n",
    "    model_tags = []\n",
    "    gt_tags = []\n",
    "    \n",
    "    for sentence, tags in val_data:\n",
    "        sentence_new = prepare_sequence(sentence, word_to_idx)\n",
    "        pred_tags = model(sentence_new)\n",
    "        pred_tags = torch.argmax(pred_tags, dim = 1).tolist()\n",
    "        pred_tags = [idx_to_tag[tag_id] for tag_id in pred_tags]\n",
    "        \n",
    "        word_list += sentence\n",
    "        model_tags += pred_tags\n",
    "        gt_tags += tags\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    errors = {}\n",
    "    for index, model_tag in enumerate(model_tags):\n",
    "        gt_tag = gt_tags[index]\n",
    "        word = word_list[index]\n",
    "        if model_tag == gt_tag:\n",
    "            continue\n",
    "        if (model_tag, gt_tag) not in errors:\n",
    "            errors[(model_tag, gt_tag)] = (0, set())\n",
    "        curr_count, curr_word_list = errors[(model_tag, gt_tag)]\n",
    "        curr_word_list.add(word)\n",
    "        errors[(model_tag, gt_tag)] = (curr_count + 1, curr_word_list)\n",
    "    errors = sorted(errors.items(), key = lambda x : x[1][0], reverse = True)[0:10]\n",
    "    return errors\n",
    "\n",
    "bilstm_word_list, bilstm_model_tags, bilstm_gt_tags = generate_predictions(bilstm_model, val_data)\n",
    "bilstm_errors = error_analysis(bilstm_word_list, bilstm_model_tags, bilstm_gt_tags)\n",
    "print(\"gt_tag\\t| model_tag\\t| freq.\\t| examples\")\n",
    "horizontal_line = \"-------------------------------------------------------------------------\"\n",
    "print(horizontal_line)\n",
    "for error in bilstm_errors:\n",
    "    tags, info = error\n",
    "    model_tag, gt_tag = tags\n",
    "    num_errors, example_words = info\n",
    "    print(\"{}\\t| {}\\t\\t| {}\\t| {}\\n{}\".format(gt_tag, model_tag, num_errors, random.sample(example_words, 5), horizontal_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl2C26leFihB"
   },
   "source": [
    "### Error analysis\n",
    "**Report your findings here.**  \n",
    "Compare the top-10 errors made by this modified model with the errors made by the model from part (a). \n",
    "If you tried multiple hyperparameter combinations, choose the model with the highest validation data accuracy.\n",
    "What errors does the original model make as compared to the modified model, and why do you think it made them? \n",
    "\n",
    "Feel free to reuse the methods defined above for this purpose.\n",
    "\n",
    "I chose the model with 92% validation accuracy with an embedding dimension of 500 and hidden dimension of 450.\n",
    "\n",
    "Once again, the same types of errors persist in this model. However, if we were to compare this model's errors to the character model and the original model, these errors are a lot more similar to the original model, which makes sense because the original model and this model do not take into account character embeddings, as such, they will naturally have more similar errors.\n",
    "\n",
    "In terms of error analysis, when comparing to the original model, this model's top 10 errors are the exact same as the original model's top 10 errors, just in a slightly different order. It is important to note the frequencies of the errors are similar as well, which makes sense, since they achieved similar validation and test accuracies (86% and 89% test accuracies respectively for the original and Bi-LSTM model). All in all, the reason these errors agree with the original model is because this model is literally the information from the original model, but in two different forms (forward dependencies, which are in the original model, and then backward dependencies, which this model adds).\n",
    "\n",
    "The only genuine difference I can see between the two models' errors is a slightly lower frequency for ground-truth JJ being predicted as NNP. This partially makes sense because adjectives can be placed on either side of a noun (e.g. \"the heroic warrior fought ferociously\" and \"the warrior heroicly fought\"). In this aforementioned example, the forward dependency will tag an adjective or verb coming right after the noun. However, in the Bi-LSTM, a backward dependency is added, which, in addition to the forward dependency, gathers an adjective or verb can come before the noun. As such, it is important to distinguish between the two cases (a word coming before another word, and a word coming after another word). Since the Bi-LSTM gathers both these dependencies, that might be why there are fewer errors in the (JJ, NNP) scenario. Similar reasoning can be used in the case of (VBN, VBD), which has a frequency of 168 in this model and a frequency of 181 in the original model. The use of neighboring words on both sides of a verb can make the difference between past tense and past participle. As such, these extra dependencies seem like they slightly reduce errors in select groups compared to the original model.\n",
    "\n",
    "In summary, the Bi-LSTM model is pretty similar to the original model, just with the backwards dependency encoded now. As mentioned above, this leads to a reduction of errors in certain tag error groups, since neighboring words in both directions of the sentence makes a difference in terms of meaning/context of a sentence, and which tag belongs to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POS_tagging.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

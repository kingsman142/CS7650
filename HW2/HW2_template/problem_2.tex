\textbf{[CS 7650 Only]} 

Assume that you are training several logistic regression models.
After training on the same data, $\hat{\theta}$ is the optimal weight for an unregularized logistic regression model and $\theta^{*}$ is the optimal weight for a logistic regression model with L2 regularization.
Prove that $||\theta^{*}||^{2}_{2} \leq ||\hat{\theta}||^{2}_{2}$. \\
\emph{Note}: you may find it useful to look at the likelihood equations for regularized and unregularized logistic regression.
[5 pts]

\begin{solution} \ \\
	Let $L(\theta) = -\log(y \vert x; \theta)$ be the negative log-likelihood loss function for logistic regression, while $L_{\text{reg}}(\theta) = L(\theta) + R(\theta) = -\log(y \vert x; \theta) + \lambda||\theta||^2_2$ is the L2 regularized version of the previous loss function ($R(\theta)$ indicates the regularization term).
	
	Let $\hat{\theta} = \text{argmin}_\theta \thinspace\thinspace L(\theta)$.
	
	Let $\theta^* = \text{argmin}_{\theta} \thinspace\thinspace L_{\text{reg}}(\theta)$.
	
	We know $L(\hat{\theta}) \leq L(\theta^*)$ since $\hat{\theta}$ is a global minimum for the non-regularized loss function. Conversely, we know $L_{\text{reg}}(\theta^*) \leq L_\text{reg}(\hat{\theta})$ since $\theta^*$ is a global minimum for the regularized loss function.
	
	Since $L_{\text{reg}}(\theta^*) \leq L_\text{reg}(\hat{\theta})$, we get the following:
	
	$L_{\text{reg}}(\theta^*) \leq L_\text{reg}(\hat{\theta})$
	
	$L(\theta^*) + R(\theta^*) \leq L(\hat{\theta}) + R(\hat{\theta})$
	
	$L(\theta^*) + ||\theta^*||^2_2 \leq L(\hat{\theta}) + ||\hat{\theta}||^2_2$
	
	We have already shown above $L(\hat{\theta}) \leq L(\theta^*)$, so we can subtract both loss function values from the inequality and it will still hold:
	
	$||\theta^*||^2_2 \leq ||\hat{\theta}||^2_2$
	
	$\therefore ||\theta^{*}||^{2}_{2} \leq ||\hat{\theta}||^{2}_{2}$\\
\end{solution}
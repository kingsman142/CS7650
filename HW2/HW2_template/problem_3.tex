Language Modeling is the technique that allows us to compute the probabilities of word sequences. The probability of a sequence $\textbf{W} = w_1^{n} = \{w_1, w_2 ... w_n\}$, with the use of chain rule, can be estimated as the product of probabilities of each word given the history, as shown-
\begin{align*}
    P(\textbf{W}) &= P(w_1, w_2 ... w_n)\\
    &= P(w_1) \; P(w_2 | w_1) \; P(w_3 | w_1, w_2) ... P(w_n | w_1, w_2 ... w_{n-1})\\
    &= \prod_{i=1}^{n} P(w_i | w_1^{i-1}) 
\end{align*}

\begin{enumerate}
    \item Using an n-gram model allows us to approximate the above probability using only a subset of of $n - 1$ words from the history at each step. Simplify the above expression for the general n-gram case, and the bi-gram case. [3 pts]
    \item A common way to have markers for the start and the end of sentence is to add the [BOS] (beginning of sentence) and [EOS] (end of sentence) tokens at the start and end of every sentence. Consider the following text snippet-
    \begin{adjustwidth}{50pt}{50pt}
    [BOS] i made cheese at home [EOS] \newline
    [BOS] i like home made cheese [EOS] \newline
    [BOS] cheese made at home is tasty [EOS] \newline
    [BOS] i like cheese that is salty [EOS] 
    \end{adjustwidth}
    Using the expression derived in (a), find the probability of the following sequence as per the bi-gram model- $P(\text{[BOS] I like cheese made at home [EOS]})$. [5 pts]
    \item In practice, instead of raw probability, perplexity is used as the metric for evaluating a language model. Define perplexity and find the value of perplexity for the sequence in (b) for the bi-gram case. [2 pts]
    \item One way to deal with unseen word arrangements in the test set is to use Laplace smoothing, which adds 1 to all bi-gram counts, before we normalize them into probabilities. An alternative to Laplace smoothing (add-1 smoothing) is add-k smoothing, where k is a fraction that allows assigning a lesser probability mass to unseen word arrangements. Find the probability of the sequence in (b) with add-k smoothing for $k=0.1$. [5 pts]
    \item To deal with unseen words in the test set, a common way is to fix a vocabulary by thresholding on the frequency of words, and assigning an [UNK] token to represent all out-of-vocabulary words. In the example from (a), use a threshold of $count > 1$ to fix the vocabulary. Find the probability for the following sequence for an add-0.1 smoothed bi-gram model- $P(\text{[BOS] i like pepperjack cheese [EOS]})$. [5 pts]
\end{enumerate}

\begin{solution} \ \\
	a) n-gram case:
	
	$P(\textbf{W}) = \prod_{i=n}^{N} P(w_i | w_{i-n+1}^{i-1})$\\
	
	bi-gram case:
	
	$P(\textbf{W}) = \prod_{i=1}^{N} P(w_i | w_{i-1}) = P(w_1)P(w_2 \vert w_1)...P(w_N \vert w_{N-1})$
	
	Note: in the bigram model, when $i = 1$, the token is the start token, which in this question is denoted as [BOS]. Similarly, in the n-gram case, the token located at $w_1$ is [BOS] as well.
	
	Note: On Piazza, the TAs clarified ``only valid n-grams'' are used in the n-gram case, so that is why the product begins at $i = n$.
	
	b) $\scriptsize P(S) = P(\text{I} \vert \text{[BOS]}) P(\text{like} \vert \text{I})P(\text{cheese} \vert \text{like}) P(\text{made} \vert \text{cheese}) P(\text{at} \vert \text{made}) P(\text{home} \vert \text{at}) P(\text{[EOS]} \vert \text{home})$\\
	$= 0.75 \cdot \frac{2}{3} \cdot 0.5 \cdot 0.25 \cdot 0.33 \cdot 1 \cdot 0.33\\
	= 0.00694$
	
	c) Perplexity is the inverse probability of a test sentence, which is then normalized by the sentence's length. The normalization is used to control for the nature of long sentences, where many more probabilities are multiplied together than a short sentence (so, the model would be biased toward shorter sentences without the normalization). The equation is as follows:
	
	$\text{PP}(S) = \sqrt[N]{\frac{1}{P(S)}} = \sqrt[N]{\frac{1}{P(w_1, w_2, \dots, w_n)}}\\
	= \sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_i \vert w_{i-1})}} \qquad$ (for the bi-gram case)
	
	From the above equation, it is easy to see why low perplexity is good and high perplexity is bad (we are using $\frac{1}{P(w_i \vert w_{i-1})}$ instead of $P(w_i \vert w_{i-1})$ directly). For the sentence given in part (b), the perplexity is as follows for the bi-gram model:
	
	$\text{PP}(S) = \sqrt[N]{\frac{1}{P(\text{I} \vert \text{[BOS]})}\frac{1}{P(\text{like} \vert \text{I})}\frac{1}{P(\text{cheese} \vert \text{like})}\frac{1}{P(\text{made} \vert \text{cheese})}\frac{1}{P(\text{at} \vert \text{made})}\frac{1}{P(\text{home} \vert \text{at})}\frac{1}{P(\text{[EOS]} \vert \text{home})}}\\$
	$= \sqrt[8]{\frac{1}{P(\text{I} \vert \text{[BOS]}) P(\text{like} \vert \text{I})P(\text{cheese} \vert \text{like}) P(\text{made} \vert \text{cheese}) P(\text{at} \vert \text{made}) P(\text{home} \vert \text{at}) P(\text{[EOS]} \vert \text{home})}}\\
	= \sqrt[8]{\frac{1}{0.00694}}\\
	= \sqrt[8]{144.0922}\\
	= 1.8613$\\

d) $\scriptsize P(S) = P(\text{I} \vert \text{[BOS]}) P(\text{like} \vert \text{I}) P(\text{cheese} \vert \text{like}) P(\text{made} \vert \text{cheese}) P(\text{at} \vert \text{made}) P(\text{home} \vert \text{at}) P(\text{[EOS]} \vert \text{home})\\
= \frac{3 + 0.1}{4 + 0.1\cdot 12} \frac{2 + 0.1}{3 + 0.1\cdot 12} \frac{1 + 0.1}{2 + 0.1\cdot 12} \frac{1 + 0.1}{4 + 0.1\cdot 12} \frac{1 + 0.1}{0.3 + 0.1\cdot 12} \frac{2 + 0.1}{2 + 0.1\cdot 12} \frac{1 + 0.1}{3 + 0.1\cdot 12}\\
= (0.5962)(0.5)(0.3438)(0.2115)(0.2619)(0.6563)(0.2619)\\
= 0.000978\\$

e) Using the examples from (b), we can construct a training set of vocabulary. As such, all the words with count $>$ 1 are ``[BOS]'', ``[EOS]'', ``i'', ``made'', ``cheese'', ``at'', ``home'', ``like'', and ``is''. All other tokens should be considered out-of-vocabulary ([UNK]). As such, the sentence provided to us, ``[BOS] i like pepperjack cheese [EOS]'', is transformed into ``[BOS] i like [UNK] cheese [EOS]''. Therefore, to calculate $P($[BOS] i like [UNK] cheese [EOS]$)$ with add-0.1 smoothing in a bi-gram model, we can combine what we have learned from previous parts of this question to create the following formula:

$\scriptsize P(S) \\
= P(\text{I} \vert \text{[BOS]}) P(\text{like} \vert \text{I}) P(\text{[UNK]} \vert \text{like}) P(\text{cheese} \vert \text{[UNK]}) P(\text{[EOS]} \vert \text{cheese})\\
= \frac{3 + 0.1}{4 + 0.1\cdot 10} \frac{2 + 0.1}{3 + 0.1\cdot 10} \frac{0 + 0.1}{2 + 0.1\cdot 10} \frac{0 + 0.1}{3 + 0.1\cdot 10} \frac{1 + 0.1}{4 + 0.1\cdot 10}\\
= (0.62)(0.525)(0.0333)(0.025)(0.22)\\
= 0.0000597$
\end{solution}
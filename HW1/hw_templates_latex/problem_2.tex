The entropy of a discrete random variable $X$ is defined as (use  base $e$ for all $\log$ operations unless specified otherwise):
\begin{equation*}
    H(X) = - \sum_{x \in X} P(x) \log P(x)
\end{equation*}

\begin{enumerate}
    \item Compute the entropy of the distribution $P(x) = \text{Multinomial}([0.2, 0.3, 0.5])$. [3 pts]
    \item Compute the entropy of the uniform distribution $P(x) = \frac{1}{m} \forall x \in [1,m]$. [3 pts]
    \item Consider the entropy of the joint distribution P(X, Y):
    \begin{equation*} 
        H(X, Y) = - \sum_{x \in X} \sum_{y \in Y} P(x, y) \log P(x, y)
    \end{equation*}
    How does this entropy relate to H(X) and H(Y), (i.e. the entropies of the marginal distributions) when X and Y are independent? [4 pts]
\end{enumerate}

\begin{solution} \ \\
a) $H(X) = - (0.2\cdot \log(0.2) + 0.3\cdot\log(0.3) + 0.5\cdot\log(0.5)) = - (-1.03) = 1.03$\\

b) $H(X) = - \sum_{i=1}^{m} P(x_i)\log P(x_i) = - \sum_{i=1}^{m} \frac{1}{m}\log \frac{1}{m} = - m \cdot \frac{1}{m}\log \frac{1}{m} = -\log\frac{1}{m} = -\log(1) + \log(m) = \log(m)$\\

c)\\
$H(X, Y) = - \sum_{x \in X}\sum_{y\in Y} P(x, y)\log P(x, y)\\
= - \sum_{x \in X}\sum_{y\in Y} P(x)P(y)\log \big[P(x)P(y)\big]\\
= - \sum_{x \in X}\sum_{y\in Y} P(x)P(y)\big[\log P(x) + \log P(y)\big]\\
= - \sum_{x \in X}\sum_{y\in Y} P(x)P(y)\log P(x) + P(x)P(y)\log P(y)\\
= - \sum_{x \in X}\sum_{y\in Y} P(x)P(y)\log P(x) - \sum_{x \in X}\sum_{y\in Y} P(x)P(y)\log P(y)\\
= - \sum_{x \in X}P(x)\log P(x)\sum_{y\in Y} P(y) - \sum_{y \in Y}P(y)\log P(y)\sum_{x\in X} P(x)\\
= - \sum_{x \in X}P(x)\log P(x) - \sum_{y \in Y}P(y)\log P(y)\\
= H(X) + H(Y)$

$\therefore$ H(X, Y) = H(X) + H(Y) if X and Y are independent
\end{solution}